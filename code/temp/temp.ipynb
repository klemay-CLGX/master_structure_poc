{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "\n",
    "world = gpd.read_file(url)\n",
    "\n",
    "world.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location = [0,0], zoom_start=2)\n",
    "m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(location, zoom_start=10):\n",
    "    map_object = folium.Map(location=location, zoom_start=zoom_start)\n",
    "    return map_object\n",
    "\n",
    "usa_map = create_map(location=[37.0902,-95.7129], zoom_start=4)\n",
    "usa_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boulder_map = create_map(location=[40.015,-105.27], zoom_start=12)\n",
    "boulder_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_map.save('usa_map.html')\n",
    "boulder_map.save('boulder.map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'City': ['Denver','Austin'],\n",
    "    'Latitude': [39.7392, 30.2672],\n",
    "    'Longitude': [-104.9903,-97.7431]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['Longitude'],data['Latitude']))\n",
    "coords_denver = (gdf.loc[0,'Latitude'],gdf.loc[0,'Longitude'])\n",
    "coords_austin = (gdf.loc[1,'Latitude'],gdf.loc[1,'Longitude'])\n",
    "\n",
    "distance = geodesic(coords_denver, coords_austin).kilometers\n",
    "print(f\"distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = gdf['Latitude'].mean()\n",
    "lon = gdf['Longitude'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = [lat,lon]\n",
    "m = folium.Map(location=midpoint, zoom_start=4)\n",
    "\n",
    "for idx, row in gdf.iterrows():\n",
    "    folium.Marker(\n",
    "        location = [row['Latitude'],row['Longitude']],popuu=row['City']\n",
    "    ).add_to(m)\n",
    "\n",
    "line = folium.PolyLine(locations=[coords_denver, coords_austin],\n",
    "                        color='blue',\n",
    "                        weight=2.5,\n",
    "                        opacity=1)\n",
    "\n",
    "m.add_child(line)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium as fl\n",
    "from streamlit_folium import st_folium\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pos(lat,lng):\n",
    "    return lat,lng\n",
    "\n",
    "m = fl.Map()\n",
    "\n",
    "m.add_child(fl.LatLngPopup())\n",
    "\n",
    "map = st_folium(m, height=350, width=700)\n",
    "\n",
    "\n",
    "data = get_pos(map['last_clicked']['lat'],map['last_clicked']['lng'])\n",
    "\n",
    "if data is not None:\n",
    "    st.write(data)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map()\n",
    "m.add_child(folium.LatLngPopup())\n",
    "st_folium(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_4 = folium.Map(location=[46.8527, -121.7649],\n",
    "                   zoom_start=13)\n",
    "folium.Marker(location=[46.8354, -121.7325], popup='Camp Muir').add_to(map_4)\n",
    "map_4.click_for_marker(popup='Waypoint')\n",
    "#map_4.create_map(path='mtrainier.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_3 = folium.Map(location=[46.1991, -122.1889],\n",
    "                   zoom_start=13)\n",
    "folium.LatLngPopup().add_to(map_3)\n",
    "map_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_coder\")\n",
    "address = \"1600 Pennsylvania Avenue NW, Washington, DC 20500, USA\"\n",
    "\n",
    "try:\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        print(f'Address: {address}')\n",
    "        print(f'Latitude: {location.latitude}, Longitude: {location.longitude}')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = geolocator.geocode(address)\n",
    "print(location)\n",
    "\n",
    "print(f'Address: {address}')\n",
    "print(f'Latitude: {location.latitude}, Longitude: {location.longitude}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker, CircleMarker, basemaps, LayersControl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import great_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_city(city_name):\n",
    "    geolocator = Nominatim(user_agent=\"geo_analysis\")\n",
    "    location = geolocator.geocode(city_name)\n",
    "    return(location.latitude, location.longitude)\n",
    "\n",
    "def cal_distance(point1, point2):\n",
    "    return great_circle(point1, point2).kilometers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"enter city name to find nearby big cities:\")\n",
    "    city_coords = geocode_city(city_name)\n",
    "    print(f\"Coordiantes of {city_name}: {city_coords}\")\n",
    "    big_cities = {\n",
    "        \"New York\": (40.7128, -74.0060),\n",
    "        \"Los Angeles\": (38.0522, -118.2437),\n",
    "        \"Chicago\": (41.8781, -87.6298)\n",
    "    }\n",
    "\n",
    "\n",
    "nearby_cities = {}\n",
    "for city, coords in big_cities.times():\n",
    "    distance = cal_distance(city_coords, coords)\n",
    "    if distance <= 500:\n",
    "        nearby_cities[city] = coords\n",
    "\n",
    "m = Map(center=(city_coords[0], city_coords[1]), zoom=8, basemaps= basemaps.OpenStreetMap.Mapnik)\n",
    "input_city_marker = Marker(location=(city_coords[0],city_coords[1]), title=city_name)\n",
    "m.add_layer(input_city_marker)\n",
    "for city, coords in nearby_cities.items():\n",
    "    marker = CircleMarker(location=(city_coords[0],city_coords[1]),radius=7, color = \"blue\", fill_color = \"blue\", fill_opacity=0.6)\n",
    "    m.add_layer(marker)\n",
    "m.add_control(LayersControl())\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Population Density Project Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "import branca.colormap as cm\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = pd.read_csv('usa_population_2019.csv')\n",
    "pop_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('us-states.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = gdf.merge(pop_data, left_on=\"name\", right_on=\"Geographic Area\")\n",
    "merged_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[\"area_km2\"] = merged_data[\"geometry\"].to_crs(epsg=3395).area /10**6\n",
    "merged_data[\"population_density\"] = merged_data[\"Total Resident Population\"]/merged_data[\"area_km2\"]\n",
    "print(merged_data[['name','Total Resident Population','area_km2','population_density']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_density = merged_data['population_density'].min()\n",
    "max_density = merged_data['population_density'].max()\n",
    "\n",
    "colormap = cm.linear.YlOrRd_09.scale(min_density, max_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_func(feature):\n",
    "    density = feature['properties']['population_density']\n",
    "    return {\n",
    "        'fillColor': colormap(density),\n",
    "        'color':'black',\n",
    "        'weight':1,\n",
    "        'fillOpacity': 0.6,\n",
    "    }\n",
    "\n",
    "m = folium.Map(location=[37.8,-96], zoom_start=4)\n",
    "folium.GeoJson(\n",
    "    merged_data,\n",
    "    style_function=style_func,\n",
    "    tooltip=GeoJsonTooltip(fields=['name','population_density'],\n",
    "                            aliases=['State:','Population Density:'],\n",
    "                            localize=True)\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = \"Population Density\"\n",
    "colormap.add_to(m)\n",
    "m.save('us_popdensity.html')\n",
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap: Air Quality Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap, HeatMapWithTime\n",
    "import pandas as pd\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ad_viz_plotval_data.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "data[\"Week\"] = data[\"Date\"].dt.isocalendar().week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_mean_aqi = data.groupby([\"Local Site Name\",\"Week\"]).agg({\n",
    "    'Site Latitude': 'first',\n",
    "    'Site Longitude': 'first',\n",
    "    'Daily AQI Value': lambda x: np.mean(x)\n",
    "}).reset_index()\n",
    "\n",
    "print('Weekly Mean AQI Value per Location:')\n",
    "for location in weekly_mean_aqi[\"Local Site Name\"].unique():\n",
    "    location_data = weekly_mean_aqi[weekly_mean_aqi['Local Site Name'] == location]\n",
    "    print(f'Location:{location}')\n",
    "    for index, row in location_data.iterrows():\n",
    "        print(f'Week: {row[\"Week\"]}, Average AQI Wale: {row[\"Daily AQI Value\"]:.2f}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_week_list = []\n",
    "\n",
    "for week in range(1,10):\n",
    "    weekly_data = weekly_mean_aqi[weekly_mean_aqi[\"Week\"]==week][[\"Site Latitude\", \"Site Longitude\",\"Daily AQI Value\"]]\n",
    "    df_week_list.append(weekly_data.values.tolist())\n",
    "\n",
    "basemap = folium.Map(location=[34.0549,-118.24], zoom_start=11)\n",
    "HeatMapWithTime(df_week_list, radius=70, gradient={0.05:'blue',0.5:'green',0.75:'yellow',1.0:'red'},\n",
    "                min_opacity=0.5,\n",
    "                max_opacity=0.8,\n",
    "                use_local_extrema=True).add_to(basemap)\n",
    "\n",
    "# Add the title\n",
    "loc = 'Weekly AQI'\n",
    "title_html = '''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{}</b></h3>\n",
    "             '''.format(loc) \n",
    "\n",
    "basemap.get_root().html.add_child(folium.Element(title_html))\n",
    "basemap.save('weekly_aqi_heatmap.html')\n",
    "\n",
    "basemap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Risk Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AEGISDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns=['lat','lon','flood_heig','elevation','precipitat']\n",
    "data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors = 'coerce')\n",
    "data_clean = data.dropna()\n",
    "print(\"Nan after cleaning:\")\n",
    "print(data_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Normalized_Height\"]= (data_clean['flood_heig']-data_clean['flood_heig'].min())/(data_clean['flood_heig'].max() - data_clean['flood_heig'].min())\n",
    "data_clean['Normalized_Elev'] = (data_clean['elevation']-data_clean['elevation'].min())/(data_clean['elevation'].max() - data_clean['elevation'].min())\n",
    "data_clean['Normalized_Precip'] = (data_clean['precipitat']-data_clean['precipitat'].min())/(data_clean['precipitat'].max() - data_clean['precipitat'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Normalized_Elev'] = 1- data_clean[\"Normalized_Elev\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Risk_Score'] = (data_clean[\"Normalized_Height\"] * 0.4 + data_clean['Normalized_Elev'] * 0.3 + data_clean['Normalized_Precip'] * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\Risk Score for Each LOcation:')\n",
    "for index, row in data_clean.iterrows():\n",
    "    print(f'Latitude:{row[\"lat\"]}, Longitude:{row[\"lon\"]}, Risk Score:{row[\"Risk_Score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(risk_score):\n",
    "    if risk_score < 0.2: \n",
    "        return 'green'\n",
    "    elif 0.2 <= risk_score < 0.4:\n",
    "        return 'yellow'\n",
    "    elif 0.4 <= risk_score < 0.6:\n",
    "        return 'orange'\n",
    "    elif 0.6 <= risk_score < 0.8:\n",
    "        return 'red'\n",
    "    else:\n",
    "        return 'darkred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['color'] = data_clean['Risk_Score'].apply(get_color)\n",
    "\n",
    "manila_coords = [14.599, 120.98]\n",
    "\n",
    "manila_map = folium.Map(location=manila_coords, zoom_start=11)\n",
    "heat_data = data_clean[['lat','lon','Risk_Score']].values.tolist()\n",
    "HeatMap(heat_data, radius=15, max_zoom=15).add_to(manila_map)\n",
    "#for index, row, in data_clean.iterrows():\n",
    "#    print((f'Latitude:{row[\"lat\"]}, Longitude:{row[\"lon\"]}, Risk Score:{row[\"Risk_Score\"]}, Risk Color:{row[\"color\"]}'))\n",
    "\n",
    "manila_map.save('flood_risk_heatmap.html')\n",
    "manila_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HeatMap Snow Cover Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_data = pd.read_csv('nyc_Jan_Jun_2016_weat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_data[\"SNOW\"] = pd.to_numeric(ny_data[\"SNOW\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_pos = ny_data[ny_data[\"SNOW\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_snow_per_loc = snow_pos.loc[snow_pos.groupby(['LATITUDE','LONGITUDE'])['SNOW'].idxmax()]\n",
    "print(\"\\nSnow Depth in Inches:\")\n",
    "for index,row in max_snow_per_loc.iterrows():\n",
    "    print(f'Latitude:{row[\"LATITUDE\"]},Longitude:{row[\"LONGITUDE\"]}, Snow Depth:{row[\"SNOW\"]:.2f}')\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"snow_depth_locator\")\n",
    "top_3 = max_snow_per_loc.nlargest(3,\"SNOW\")\n",
    "print(\"\\n Top 3 Places\")\n",
    "for index, row in top_3.iterrows():\n",
    "    location = geolocator.reverse((row['LATITUDE'], row[\"LONGITUDE\"]),exactly_one=True)\n",
    "    address = location.address if location else \"Address not found\"\n",
    "    print(f'Latitude:{row[\"LATITUDE\"]},Longitude:{row[\"LONGITUDE\"]}, Address: {address}, Snow Depth:{row[\"SNOW\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_snow = ny_data[\"SNOW\"].min()\n",
    "print(min_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_data = max_snow_per_loc[['LATITUDE','LONGITUDE','SNOW']].values.tolist()\n",
    "map_center = [40.71,-74.006]\n",
    "map_nyc = folium.Map(location=map_center, zoom_start=11)\n",
    "HeatMap(heat_data, radius=45).add_to(map_nyc)\n",
    "\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open street stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import osmnx as ox \n",
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = [40.73, -74.99]\n",
    "dest =   [40.81, -73.96]\n",
    "rd = ox.graph_from_place('Manhattan, New York, USA', network_type='drive')\n",
    "origin_node = ox.distance.nearest_nodes(rd, origin[1], origin[0])\n",
    "dest_node = ox.distance.nearest_nodes(rd, dest[1], dest[0])\n",
    "shortest_path = nx.shortest_paths(rd, origin_node, dest_node, weight='length')\n",
    "path_coords = [(rd.nodes[node]['y'],rd.nodes[node]['x']) for node in shortest_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_2024_v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
